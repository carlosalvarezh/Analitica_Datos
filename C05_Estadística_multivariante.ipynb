{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dbdba8d",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Analítica de datos para la toma de decisiones empresariales</h1>\n",
    "<h1 align=\"center\">Estadística multivariante</h1>\n",
    "<h1 align=\"center\">Centro de Educación Continua</h1>\n",
    "<h1 align=\"center\">EAFIT</h1>\n",
    "<h1 align=\"center\">2023</h1>\n",
    "<h1 align=\"center\">MEDELLÍN - COLOMBIA </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0b6436",
   "metadata": {},
   "source": [
    "*** \n",
    "|![Gmail](https://img.shields.io/badge/Gmail-D14836?style=plastic&logo=gmail&logoColor=white)|<carlosalvarezh@gmail.com>|![Outlook](https://img.shields.io/badge/Microsoft_Outlook-0078D4?style=plastic&logo=microsoft-outlook&logoColor=white)|<calvar52@eafit.edu.co>|\n",
    "|-:|:-|--:|:--|\n",
    "|[![LinkedIn](https://img.shields.io/badge/linkedin-%230077B5.svg?style=plastic&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/carlosalvarez5/)|[![@alvarezhenao](https://img.shields.io/twitter/url/https/twitter.com/alvarezhenao.svg?style=social&label=Follow%20%40alvarezhenao)](https://twitter.com/alvarezhenao)|[![@carlosalvarezh](https://img.shields.io/badge/github-%23121011.svg?style=plastic&logo=github&logoColor=white)](https://github.com/carlosalvarezh)|[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/carlosalvarezh/Curso_CEC_EAFIT/blob/main/C05_Estadística_multivariante.ipynb)|\n",
    "\n",
    "<table>\n",
    " <tr align=left><td><img align=left src=\"https://github.com/carlosalvarezh/Curso_CEC_EAFIT/blob/main/images/CCLogoColorPop1.gif?raw=true\" width=\"25\">\n",
    " <td>Text provided under a Creative Commons Attribution license, CC-BY. All code is made available under the FSF-approved MIT license.(c) Carlos Alberto Alvarez Henao</td>\n",
    "</table>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bce07bf",
   "metadata": {},
   "source": [
    "## Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69274ee9",
   "metadata": {},
   "source": [
    "La [Estadística Multivariante](https://en.wikipedia.org/wiki/Multivariate_statistics) es una rama avanzada de la estadística que se enfoca en el estudio y análisis de conjuntos de datos que involucran múltiples variables simultáneamente. A diferencia de la estadística univariante que se concentra en una única variable, la estadística multivariante aborda la complejidad de las relaciones entre dos o más variables y busca descubrir patrones y estructuras ocultas en los datos.\n",
    "\n",
    "En la estadística multivariante, se desarrollan diversos tipos de análisis que nos permiten explorar, interpretar y tomar decisiones basadas en conjuntos de datos multivariantes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95d9914",
   "metadata": {},
   "source": [
    "## Visualización de datos multivariantes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320413fe",
   "metadata": {},
   "source": [
    "### Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c424fb83",
   "metadata": {},
   "source": [
    "La visualización de datos multivariantes es una poderosa herramienta dentro del análisis de datos, que nos permite explorar y comprender las relaciones complejas entre múltiples variables de manera simultánea. En este capítulo, exploraremos los conceptos fundamentales de la visualización de datos multivariantes y aprenderemos a utilizar diferentes tipos de gráficos y técnicas para representar y analizar conjuntos de datos con varias variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e86b744",
   "metadata": {},
   "source": [
    "### Desafíos en la visualización de datos multivariantes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04033553",
   "metadata": {},
   "source": [
    "La visualización de datos multivariados enfrenta los mismos desafíos que la visualización de información: encontrar representaciones visuales adecuadas para un problema puede ser difícil y no determinista. Además, los datos multivariados plantean problemas en la codificación de sus atributos en una única presentación visual."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2c7e24",
   "metadata": {},
   "source": [
    "#### Mapeo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc22b9c",
   "metadata": {},
   "source": [
    "Encontrar un mapeo adecuado de datos multivariados de alta dimensionalidad en una forma visual en 2D nunca es una tarea sencilla. Por lo general, depende de la naturaleza de los conjuntos de datos que se van a visualizar y está más relacionado con la percepción humana. Además, la asociación de atributos de datos con entidades gráficas requiere extrema precaución para evitar abrumar la capacidad de visualización del observador. La conjunción de varios elementos en las representaciones puede provocar una sobrecarga cognitiva en los usuarios, por lo que los atributos gráficos deben seleccionarse cuidadosamente para que sean fáciles de desentrañar. Es importante que los diferentes atributos puedan ser vistos de manera holística para un análisis integrado y, al mismo tiempo, que cada dimensión pueda ser evaluada por separado e independientemente por los usuarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522950ba",
   "metadata": {},
   "source": [
    "#### Dimensionalidad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e50259",
   "metadata": {},
   "source": [
    "Los datos multivariados suelen tener un tamaño enorme y una alta dimensionalidad que probablemente resultará en una estructura densa. Por lo tanto, es difícil presentar tales datos en una única representación visual, lo que dificulta permitir que los usuarios exploren el espacio de datos de manera intuitiva e interactiva, así como discriminar las dimensiones individuales. Las habilidades de visualización dual y las técnicas de distorsión como los \"fisheyes\" pueden ser útiles para resolver este problema. Además, el ordenamiento de las dimensiones tiene un impacto importante en la expresividad de la visualización. Diferentes disposiciones permiten llegar a diferentes conclusiones, pero aún no se ha establecido un principio de ordenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d2d336",
   "metadata": {},
   "source": [
    "#### Dificultades en el Diseño"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd265298",
   "metadata": {},
   "source": [
    "La visualización puede ofrecer una panorámica cualitativa de conjuntos de datos grandes y complejos para que los usuarios puedan buscar estructuras, características, patrones, tendencias y relaciones de manera más efectiva. Debido a la alta dimensionalidad de los datos multivariados, inevitablemente sacrificamos la capacidad de mostrar los detalles de cada atributo, ya que tenemos menos atributos gráficos para la codificación. Esta situación puede no ser adecuada cuando se requiere un análisis cuantitativo. En la visualización de datos multivariados, siempre existe un equilibrio entre la cantidad de información, la simplicidad y la precisión."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8593316c",
   "metadata": {},
   "source": [
    "#### Evaluación de la Eficacia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94cdd02",
   "metadata": {},
   "source": [
    "El objetivo final de la visualización de datos multivariados es obtener perspicacia en los datos y mostrar la posible correlación entre diferentes atributos. En la mayoría de los casos, ciertas correlaciones aún no se han descubierto antes de observar la representación visual, y precisamente son lo que queremos adquirir después de la visualización. Es una paradoja que impide la evaluación de la eficacia de una técnica de visualización de información: no sabemos qué conocimientos valiosos están presentes en los datos, por lo que esperamos obtener perspicacia al visualizarlos. Sin embargo, si no conocemos nada sobre el patrón o la relación que se mostrará en la representación de los datos, nunca podremos evaluar la eficacia de una técnica de visualización particular."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd0a2e4",
   "metadata": {},
   "source": [
    "### Conceptos y terminología"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56f07f8",
   "metadata": {},
   "source": [
    "#### Dimensionalidad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4909b82",
   "metadata": {},
   "source": [
    "La dimensionalidad de un problema en la visualización de la información se refiere al número de atributos, o más generalmente a las variables, que se presentan en los datos que se van a visualizar. Para datos unidimensionales, que también se conocen como datos univariados, consta de un solo atributo, como una colección de casas caracterizadas por su costo. Pueden visualizarse de manera efectiva mediante herramientas tradicionales como tablas e histogramas. La interpretación de datos bidimensionales o bivariados suele utilizar las coordenadas x e y de un espacio en 2D. Un enfoque convencional consiste en trazar una variable en función de la otra, lo que se llama un gráfico de dispersión (scatterplot)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9752b10",
   "metadata": {},
   "source": [
    "#### Multidimensional y Multivariado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939a401b",
   "metadata": {},
   "source": [
    "Los términos multidimensional y multivariado a menudo se utilizan vagamente. Rigurosamente hablando, multidimensional se refiere a la dimensionalidad de las dimensiones independientes, mientras que multivariado se refiere a la de las variables dependientes. El término más apropiado para la visualización de datos multivariados debería ser visualización de datos multidimensionales multivariados. No obstante, un conjunto de datos multivariados tiene una alta dimensionalidad y posiblemente pueda considerarse multidimensional porque las relaciones clave entre los atributos generalmente no se conocen de antemano. La propiedad multidimensional está implícita en el uso común. Por conveniencia, el término atributos denota tanto las dimensiones independientes como las variables dependientes. También vale la pena señalar que la visualización de datos multivariados es bastante genérica y no se clasifica claramente entre la visualización de información y la visualización científica."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6648c2e",
   "metadata": {},
   "source": [
    "### Tipos de visualizaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f4d955",
   "metadata": {},
   "source": [
    "#### Clasificaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61b3acc",
   "metadata": {},
   "source": [
    "Las técnicas de exploración visual de datos para datos multivariados multidimensionales se pueden dividir en seis clases: técnicas geométricas, basadas en iconos, orientadas a píxeles, jerárquicas, basadas en grafos y técnicas híbridas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82031550",
   "metadata": {},
   "source": [
    "#### Matriz de Dispersión"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937c1e41",
   "metadata": {},
   "source": [
    "La matriz de dispersión es una herramienta inicial que nos permite visualizar la relación entre pares de variables en un conjunto de datos multivariantes. Cada variable se representa en un eje y los puntos en el gráfico representan las observaciones. Esto nos ayuda a identificar correlaciones y patrones generales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1f0e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "df = px.data.iris()\n",
    "fig = px.scatter_matrix(df)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d97a793",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_theme(style=\"ticks\")\n",
    "\n",
    "df = sns.load_dataset(\"penguins\")\n",
    "sns.pairplot(df, hue=\"species\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77e1632",
   "metadata": {},
   "source": [
    "#### Gráficos de burbuja"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b8d40f",
   "metadata": {},
   "source": [
    "Los gráficos de burbujas amplían la matriz de dispersión al introducir una tercera variable, representada por el tamaño de las burbujas en el gráfico. Esto permite visualizar las relaciones entre tres variables al mismo tiempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1426e775",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_theme(style=\"white\")\n",
    "\n",
    "# Load the example mpg dataset\n",
    "mpg = sns.load_dataset(\"mpg\")\n",
    "\n",
    "# Plot miles per gallon against horsepower with other semantics\n",
    "sns.relplot(x=\"horsepower\", y=\"mpg\", hue=\"origin\", size=\"weight\",\n",
    "            sizes=(40, 400), alpha=.5, palette=\"muted\",\n",
    "            height=6, data=mpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4d13be",
   "metadata": {},
   "source": [
    "#### Gráficos de Correlación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b3a6e5",
   "metadata": {},
   "source": [
    "Los gráficos de correlación, como el mapa de calor (heatmap), resaltan las relaciones de correlación entre múltiples variables. Los valores de correlación se representan mediante colores, facilitando la identificación de patrones y asociaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1922df",
   "metadata": {},
   "outputs": [],
   "source": [
    "glue = sns.load_dataset(\"glue\").pivot(\"Model\", \"Task\", \"Score\")\n",
    "sns.heatmap(glue)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fc34fb",
   "metadata": {},
   "source": [
    "Utilice `annot` para representar los valores de celda con texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf8ed90",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(glue, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc944fe5",
   "metadata": {},
   "source": [
    "Dando formato a los valores internos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d603335",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(glue, annot=True, fmt=\".2f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413ab4b1",
   "metadata": {},
   "source": [
    "#### Análisis de Componentes Principales (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2b1ece",
   "metadata": {},
   "source": [
    "[PCA](https://en.wikipedia.org/wiki/Principal_component_analysis) es una técnica que reduce la dimensionalidad de los datos multivariantes al transformar las variables originales en nuevas variables (componentes principales) que capturan la mayor variabilidad en los datos. Esto facilita la visualización en espacios de menor dimensión y puede revelar estructuras ocultas en los datos.\n",
    "\n",
    "Utilizaremos el [dataset Iris](https://en.wikipedia.org/wiki/Iris_flower_data_set) para ilustrar el ejemplo. Podemos cargar el conjunto de datos desde la librería `seaborn`. Vamos a cargar las librerías que necesitaremos, leer el dataset y mostramos las cinco primeras filas del dataset para visualizar cómo son los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce170919",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219d7076",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = sns.load_dataset(\"iris\")\n",
    "iris.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288cb41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37e5c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571dbf52",
   "metadata": {},
   "source": [
    "Uno de los requisitos para el análisis es que las variables estén correlacionadas para obtener una representación más simple de éstas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74d98b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92f7b89",
   "metadata": {},
   "source": [
    "Necesitamos que las variables numéricas estén estandarizadas porque se observa una diferencia de magnitud entre ellas. Podemos ver que petal_width y sepal_length están en magnitudes totalmente distintas, con lo cual a la hora de calcular los componentes principales sepal_length va a dominar a petal_width por la mayor escala de magnitud y, por tanto, mayor rango de varianza. \n",
    "\n",
    "***Nota:*** Estandarizar se refiere a escalar la distribución de los datos de forma tal que la media de los valores observados sea igual a 0 y su desviación estándar igual a 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2020c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.loc[:, [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]]\n",
    "Y = iris.loc[:, [\"species\"]]\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03df05c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af43fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfX = pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6abe6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfX.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8319275b",
   "metadata": {},
   "source": [
    "Una vez que tenemos nuestras variables numéricas estandarizadas, es hora de proceder con el cálculo de los componentes principales. En este caso, vamos a elegir dos componentes principales por la simple razón de que este es un ejemplo sencillo y queremos visualizar en dos dimensiones más adelante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284588b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "PCA = PCA(n_components=2)\n",
    "components = PCA.fit_transform(X)\n",
    "PCA.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9a9480",
   "metadata": {},
   "source": [
    "Para ver cómo se relacionan los componentes principales con las variables originales, mostramos los vectores propios o loadings. Vemos que el primer componente principal da casi el mismo peso a `sepal_length`, `petal_length` y `petal.width`, mientras que el segundo componente principal da peso primordialmente a `sepal_width`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db42d225",
   "metadata": {},
   "source": [
    "podemos ver con los valores propios la varianza explicada por los dos componentes principales y la varianza explicada acumulada (la explicación se verá más adelante en el tema correspondiente a Reducción de Dimensión)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f0e7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "cumVar = pd.DataFrame(np.cumsum(PCA.explained_variance_ratio_)*100, \n",
    "                      columns=[\"cumVarPerc\"])\n",
    "expVar = pd.DataFrame(PCA.explained_variance_ratio_*100, columns=[\"VarPerc\"])\n",
    "pd.concat([expVar, cumVar], axis=1)\\\n",
    "    .rename(index={0: \"PC1\", 1: \"PC2\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3a4e38",
   "metadata": {},
   "source": [
    "El primer componente principal explica un $72.96\\%$ de la variación total de los datos originales, mientras que el segundo explica un $22.85\\%$. Conjuntamente, los dos componentes principales explican alrededor del $95.81\\%$ de la variación total, un porcentaje bastante elevado. \n",
    "\n",
    "A continuación se hará la visualización de los datos. El diagrama de dispersión nos sirve para ver los valores de las observaciones respecto de los dos componentes principales y resaltamos las diferentes observaciones por su especie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9304c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "componentsDf = pd.DataFrame(data = components, columns = ['PC1', 'PC2'])\n",
    "pcaDf = pd.concat([componentsDf, Y], axis=1)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "sns.scatterplot(data=pcaDf, x=\"PC1\", y=\"PC2\", hue=\"species\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944360f7",
   "metadata": {},
   "source": [
    "## Indicadores de centralidad, dispersión y forma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43828284",
   "metadata": {},
   "source": [
    "### Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e500fe",
   "metadata": {},
   "source": [
    "En este módulo, exploraremos cómo analizar y comprender conjuntos de datos que contienen múltiples variables, lo que es esencial para la toma de decisiones empresariales informadas. Enfocaremos nuestra atención en tres aspectos clave de la estadística multivariante: centralidad, dispersión y forma de los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b283d66",
   "metadata": {},
   "source": [
    "### Indicadores de Centralidad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9265fa2f",
   "metadata": {},
   "source": [
    "La centralidad se refiere a la ubicación típica o punto central en un conjunto de datos multivariante. Aquí, analizamos no solo una variable a la vez, sino múltiples variables simultáneamente. Los indicadores más comunes de centralidad en estadística multivariante son:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f38f8a",
   "metadata": {},
   "source": [
    "#### Media Multivariante (Vector de Medias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4578f32",
   "metadata": {},
   "source": [
    "La media multivariante representa el punto promedio en todas las dimensiones del conjunto de datos. Es una extensión de la media univariante y se calcula sumando los valores de todas las variables para cada observación y luego dividiendo entre el número de observaciones.\n",
    "\n",
    "$$\\bar{\\textbf{x}}=\\frac{1}{n}\\sum \\limits_{i=1}^{n}\\textbf{x}_i$$\n",
    "\n",
    "donde $\\bar{\\textbf{x}}$ es el vector de medias y $\\textbf{x}_i$ son las observaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ef0ff5",
   "metadata": {},
   "source": [
    "Ejemplo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ebc4ba",
   "metadata": {},
   "source": [
    "#### Mediana Multivariante"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bce165b",
   "metadata": {},
   "source": [
    "La mediana multivariante es el valor que divide al conjunto de datos en dos partes iguales. A diferencia de la mediana univariante, aquí estamos considerando múltiples dimensiones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955155c9",
   "metadata": {},
   "source": [
    "### Indicadores de Dispersión"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c180b3d4",
   "metadata": {},
   "source": [
    "Los indicadores de dispersión nos ayudan a entender cuán dispersos o concentrados están los datos alrededor de su punto central. Para estadística multivariante, consideramos matrices de covarianza y correlación:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dcb78e",
   "metadata": {},
   "source": [
    "#### Matriz de Covarianza"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633121ab",
   "metadata": {},
   "source": [
    "La matriz de covarianza muestra cómo varían conjuntamente las diferentes variables en relación con sus medias. Si dos variables tienden a aumentar juntas, su covarianza será positiva; si una aumenta mientras la otra disminuye, la covarianza será negativa.\n",
    "\n",
    "$$\\textbf{C}=\\frac{1}{n-1}\\sum \\limits_{i=1}^{n}(\\textbf{x}_i-\\bar{\\textbf{x}})(\\textbf{x}_i-\\bar{\\textbf{x}})^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96348c6b",
   "metadata": {},
   "source": [
    "#### Matriz de Correlación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ddda3f",
   "metadata": {},
   "source": [
    "La matriz de correlación es similar a la matriz de covarianza, pero normaliza los valores para que estén en el rango $[-1, 1]$. Esto permite comparar la relación entre variables sin verse afectado por las escalas individuales de las variables.\n",
    "\n",
    "$$\\textbf{R}=\\frac{\\textbf{C}}{\\sigma_i \\sigma_j}$$\n",
    "\n",
    "donde $\\sigma_i$ y $\\sigma_j$ son las desviaciones estándar de las variables $i$ y $j$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6a7a38",
   "metadata": {},
   "source": [
    "### Indicadores de Forma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee791c9c",
   "metadata": {},
   "source": [
    "Los indicadores de forma nos proporcionan información sobre la distribución y la estructura de los datos en múltiples dimensiones:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a62f6c",
   "metadata": {},
   "source": [
    "#### Elipsoides de Dispersión"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ad0280",
   "metadata": {},
   "source": [
    "Visualmente, podemos representar la dispersión de los datos en forma de elipsoides. Estos elipsoides muestran cómo los datos se extienden en diferentes direcciones en el espacio multivariante. La orientación y los ejes principales del elipsoide están relacionados con los autovectores y autovalores de la matriz de covarianza."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07083371",
   "metadata": {},
   "source": [
    "#### Coeficiente de Aplanamiento (Kurtosis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eec396a",
   "metadata": {},
   "source": [
    "- La kurtosis mide la forma de la distribución de los datos alrededor de su pico central en comparación con la distribución normal.\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "- Puede ser usado en estadística multivariante para entender cómo las variables se distribuyen en términos de su forma y la concentración de los datos alrededor de la media.\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "- Valores positivos indican una distribución más puntiaguda (leptocúrtica), mientras que valores negativos indican una distribución más aplanada (platicúrtica)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b8183e",
   "metadata": {},
   "source": [
    "#### Esfericidad de Mahalanobis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df107b86",
   "metadata": {},
   "source": [
    "- La esfericidad de Mahalanobis mide si los datos multivariados están igualmente dispersos en todas las direcciones a partir del centro de la distribución.\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "- Una esfericidad perfecta indicaría que las variables son independientes y tienen la misma varianza. La desviación de la esfericidad puede indicar correlaciones entre variables o diferencias en las varianzas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588c9af7",
   "metadata": {},
   "source": [
    "#### Índice de Forma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47477587",
   "metadata": {},
   "source": [
    "- El índice de forma mide la elongación de la distribución de los datos en relación con las variables principales de la distribución.\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "- Puede ayudar a detectar relaciones entre las variables y entender cómo se extienden los datos en diferentes direcciones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7519979d",
   "metadata": {},
   "source": [
    "### Ejemplo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01a726e",
   "metadata": {},
   "source": [
    "#### Enunciado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890b9c38",
   "metadata": {},
   "source": [
    "Supongamos que tenemos datos financieros de cinco empresas en un año, incluyendo sus ingresos, gastos y beneficios. Vamos a realizar un análisis detallado, calculando los indicadores de centralidad (media multivariante), dispersión (matriz de covarianza) y forma (elipsoides de dispersión) para comprender la estructura de estos datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f8f903",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "# Datos de ejemplo: ingresos, gastos, beneficios\n",
    "data = np.array([\n",
    "    [100000, 80000, 20000],\n",
    "    [120000, 90000, 25000],\n",
    "    [95000, 75000, 18000],\n",
    "    [105000, 85000, 22000],\n",
    "    [110000, 87000, 23000]\n",
    "])\n",
    "\n",
    "# Cálculo de la media multivariante (vector de medias)\n",
    "mean_vector = np.mean(data, axis=0)\n",
    "\n",
    "# Cálculo de la matriz de covarianza\n",
    "cov_matrix = np.cov(data, rowvar=False)\n",
    "\n",
    "# Creación de la figura\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Visualización de los datos\n",
    "ax.scatter(data[:, 0], data[:, 1], c='b', marker='o', label='Datos')\n",
    "\n",
    "# Cálculo de los autovectores y autovalores\n",
    "eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n",
    "\n",
    "# Visualización de los elipsoides de dispersión\n",
    "for i in range(1, 4):\n",
    "    eigenvalue = eigenvalues[-i]\n",
    "    eigenvector = eigenvectors[:, -i]\n",
    "    width = np.sqrt(eigenvalue) * 2\n",
    "    height = np.sqrt(eigenvalue) * 2\n",
    "    theta = np.degrees(np.arctan2(eigenvector[1], eigenvector[0]))\n",
    "    ellipse = Ellipse(mean_vector[:2], width, height, theta, edgecolor='r', facecolor='none')\n",
    "    ax.add_patch(ellipse)\n",
    "\n",
    "# Etiquetas de los ejes\n",
    "ax.set_xlabel('Ingresos')\n",
    "ax.set_ylabel('Gastos')\n",
    "\n",
    "# Título y leyendas\n",
    "ax.set_title('Elipsoides de Dispersión')\n",
    "ax.legend()\n",
    "\n",
    "# Relación de aspecto igual en ambos ejes para una visualización precisa\n",
    "ax.set_aspect('equal', 'box')\n",
    "\n",
    "# Mostrar el gráfico\n",
    "plt.show()\n",
    "\n",
    "# Imprimir resultados\n",
    "print(f\"Media Multivariante (Ingresos, Gastos, Beneficios): {mean_vector}\")\n",
    "print(\"Matriz de Covarianza:\")\n",
    "print(cov_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6ee10d",
   "metadata": {},
   "source": [
    "#### Análisis de Resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca15228a",
   "metadata": {},
   "source": [
    "***Media Multivariante:***\n",
    "\n",
    "- La media multivariante ($[104000.,83500.,21600.]$) nos indica que, en promedio, las empresas tienen ingresos cercanos a $\\$104,000$, gastos cercanos a $\\$83,500$ y beneficios de $\\$21,600$.\n",
    "- Esta información proporciona un punto central en el espacio de las tres variables.  \n",
    "\n",
    "***Matriz de Covarianza:***\n",
    "\n",
    "- La matriz de covarianza revela cómo las variables están relacionadas en términos de sus variabilidades conjuntas.\n",
    "- Los valores fuera de la diagonal principal indican las covarianzas entre las variables. Cuanto más positivo o negativo es el valor, mayor es la relación conjunta.\n",
    "- Los valores en la diagonal principal son las varianzas de cada variable.\n",
    "- Esta matriz captura la estructura de la variabilidad entre los datos financieros.\n",
    "\n",
    "***Elipsoides de Dispersión:***\n",
    "\n",
    "- Los elipsoides de dispersión nos permiten visualizar cómo los datos se extienden en el espacio de \"Ingresos\" y \"Gastos\".\n",
    "- La orientación y el tamaño de los elipsoides se relacionan con la estructura de covarianza y la variabilidad en las combinaciones de variables.\n",
    "- Los datos dentro de los elipsoides representan la variabilidad típica, mientras que los datos fuera pueden ser considerados atípicos o inusuales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdb8418",
   "metadata": {},
   "source": [
    "En conjunto, estos resultados nos proporcionan información valiosa sobre la relación entre ingresos, gastos y beneficios de las empresas. La media multivariante y los elipsoides de dispersión nos ayudan a entender el punto central y la variabilidad de los datos en este espacio multidimensional, mientras que la matriz de covarianza nos proporciona información sobre cómo se relacionan conjuntamente estas variables financieras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0544f5f",
   "metadata": {},
   "source": [
    "## Similaridad de individuos y registros (Distancia estadística)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd858664",
   "metadata": {},
   "source": [
    "### Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5377c9",
   "metadata": {},
   "source": [
    "En el análisis de datos empresariales, a menudo nos encontramos con conjuntos de datos que contienen múltiples variables relacionadas entre sí. En lugar de considerar estas variables de manera aislada, la estadística multivariante nos permite explorar las interacciones y patrones conjuntos entre ellas. Una herramienta fundamental en este contexto es la medición de la similaridad entre individuos o registros, lo que nos permite agrupar, comparar y clasificar elementos de manera significativa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff9f9a8",
   "metadata": {},
   "source": [
    "### Definición de Similaridad y Distancia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9019892f",
   "metadata": {},
   "source": [
    "La similaridad entre dos individuos o registros se refiere a qué tan parecidos son en términos de sus características multivariantes. Por otro lado, la distancia es la medida que cuantifica qué tan diferentes son dos individuos. La idea es que individuos similares tendrán distancias pequeñas, mientras que aquellos que son muy diferentes tendrán distancias grandes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1ee374",
   "metadata": {},
   "source": [
    "### Medidas de Distancia en Estadística Multivariante"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6e46d5",
   "metadata": {},
   "source": [
    "<p float=\"center\">\n",
    "  <img src=\"https://github.com/carlosalvarezh/Curso_CEC_EAFIT/blob/main/images/C04_03_Distances.webp?raw=true\" width=\"500\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb3ff72",
   "metadata": {},
   "source": [
    "#### Distancia Euclidiana"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574d95ca",
   "metadata": {},
   "source": [
    "Una de las medidas de distancia más comunes es la distancia euclidiana. \n",
    "\n",
    "<p float=\"center\">\n",
    "  <img src=\"https://github.com/carlosalvarezh/Curso_CEC_EAFIT/blob/main/images/C04_02_Euclidean.png?raw=true\" width=\"250\" />\n",
    "</p>\n",
    "\n",
    "Esta medida calcula la longitud del vector que conecta dos puntos en un espacio de características multivariadas. Para dos vectores $\\textbf{x}$ y $\\textbf{y}$ $p$-dimensiones, la distancia euclidiana se define como:\n",
    "\n",
    "$$d_M(\\textbf{x},\\textbf{y}) = \\sqrt{\\sum_{i=1}^{p}(\\textbf{x}_i-\\textbf{y}_i)^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a124ece",
   "metadata": {},
   "source": [
    "#### Distancia de Mahalanobis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a72d743",
   "metadata": {},
   "source": [
    "La distancia de Mahalanobis es una versión más sofisticada que tiene en cuenta la correlación entre las variables. Esta distancia corrige las diferencias en la escala de las variables y ajusta la medida según la estructura de covarianza de los datos. Para dos vectores $\\textbf{x}$ y $\\textbf{y}$ y con una matriz de covarianza $S$, la distancia de Mahalanobis se calcula como:\n",
    "\n",
    "$$d_M(\\textbf{x},\\textbf{y}) = \\sqrt{(\\textbf{x}-\\textbf{y})^T \\cdot \\textbf{S}^{-1} \\cdot (\\textbf{x}-\\textbf{y})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d843fd5",
   "metadata": {},
   "source": [
    "#### Distancia de Manhattan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c39fc3",
   "metadata": {},
   "source": [
    "La distancia de Manhattan es otra medida de distancia utilizada en estadística multivariante. También conocida como distancia $L_1$ o distancia de la ciudad, esta medida calcula la suma de las diferencias absolutas entre las coordenadas de dos puntos. En otras palabras, la distancia que tomaría recorrer una cuadrícula de calles en una ciudad para moverse de un punto a otro.\n",
    "\n",
    "<p float=\"center\">\n",
    "  <img src=\"https://github.com/carlosalvarezh/Curso_CEC_EAFIT/blob/main/images/C04_01_Manhattan.png?raw=true\" width=\"250\" />\n",
    "</p>\n",
    "\n",
    "Para dos vectores $\\textbf{x}$ y $\\textbf{y}$ y en $p$-dimensiones, la distancia de Manhattan se define como:\n",
    "\n",
    "$$d_{\\text{Manhattan}}(\\textbf{x},\\textbf{y})=\\sum_{i=1}^{p} |\\textbf{x}_i-\\textbf{y}_i|$$\n",
    "\n",
    "Esta distancia es especialmente útil cuando las variables tienen diferentes unidades de medida o escalas, ya que no considera las diferencias al cuadrado como en la distancia euclidiana."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3a0e1b",
   "metadata": {},
   "source": [
    "#### Distancia de Minkowski"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d86b03",
   "metadata": {},
   "source": [
    "La distancia de Minkowski es una generalización de las distancias euclidiana y de Manhattan. Se define como:\n",
    "\n",
    "$$d_M(\\textbf{x},\\textbf{y}) = \\left( \\sum_{i=1}^{p} |\\textbf{x}_i-\\textbf{y}_i|^r \\right)^{\\frac{1}{r}}$$\n",
    "\n",
    "Aquí, $r$ es un parámetro que puede ajustarse para adaptarse a diferentes situaciones. Cuando $r=2$, la distancia de Minkowski es equivalente a la distancia euclidiana, y cuando $r=1$, es equivalente a la distancia de Manhattan.\n",
    "\n",
    "La distancia de Minkowski permite variar el grado de énfasis en las diferencias más grandes entre las coordenadas (mayor \n",
    "$r$) o tratar todas las diferencias por igual (menor $r$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50737b0a",
   "metadata": {},
   "source": [
    "#### Similitud del Coseno"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5371d6",
   "metadata": {},
   "source": [
    "La similitud del coseno es una medida de similaridad que se utiliza para medir la orientación de dos vectores en un espacio multidimensional. A menudo se emplea en conjuntos de datos dispersos o en análisis de texto, como el procesamiento de lenguaje natural.\n",
    "\n",
    "La similitud del coseno se define como el coseno del ángulo entre dos vectores $\\textbf{x}$ y $\\textbf{y}$, normalizados por su magnitud:\n",
    "\n",
    "$$\\text{Similitud del Coseno}(\\textbf{x},\\textbf{y}) = \\frac{\\textbf{x} \\cdot \\textbf{y}}{||\\textbf{x}||\\cdot ||\\textbf{y}||}$$\n",
    "\n",
    "Aquí, $\\textbf{x}\\cdot \\textbf{y}$ representa el producto escalar entre los dos vectores, y $||\\textbf{x}||$ y $||\\textbf{y}||$ son las magnitudes de los vectores $\\textbf{x}\\cdot \\textbf{y}$, respectivamente.\n",
    "\n",
    "Un valor de similitud del coseno cercano a $1$ indica que los vectores están muy alineados y son similares en dirección, mientras que un valor cercano a $0$ indica que los vectores son ortogonales o no están relacionados en términos de dirección."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd834a0",
   "metadata": {},
   "source": [
    "### Ejemplo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781cd132",
   "metadata": {},
   "source": [
    "En el siguiente ejemplo, vamos a considerar cuatro características (Precio, Calidad, Popularidad e Innovación) para tres productos diferentes. Vamos a analizar los resultados de las diferentes medidas de distancia y similitud:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f68b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cityblock, minkowski\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Datos de productos en un espacio 4D (Precio, Calidad, Popularidad, Innovación)\n",
    "producto_a = np.array([50, 8, 5, 3])\n",
    "producto_b = np.array([70, 7, 8, 4])\n",
    "producto_c = np.array([60, 6, 3, 7])\n",
    "\n",
    "# Distancia de Manhattan\n",
    "dist_manhattan_ab = cityblock(producto_a, producto_b)\n",
    "dist_manhattan_ac = cityblock(producto_a, producto_c)\n",
    "print(\"Distancia de Manhattan A-B:\", dist_manhattan_ab)\n",
    "print(\"Distancia de Manhattan A-C:\", dist_manhattan_ac)\n",
    "\n",
    "# Distancia de Minkowski con r = 2 (equivalente a distancia euclidiana)\n",
    "dist_minkowski_ab = minkowski(producto_a, producto_b, 2)\n",
    "dist_minkowski_ac = minkowski(producto_a, producto_c, 2)\n",
    "print(\"Distancia de Minkowski A-B:\", dist_minkowski_ab)\n",
    "print(\"Distancia de Minkowski A-C:\", dist_minkowski_ac)\n",
    "\n",
    "# Similitud del Coseno\n",
    "matriz_productos = np.vstack((producto_a, producto_b, producto_c))\n",
    "similitud_coseno = cosine_similarity(matriz_productos)\n",
    "print(\"Similitud del Coseno:\")\n",
    "print(similitud_coseno)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caeb4bf1",
   "metadata": {},
   "source": [
    "***Análisis de Resultados***\n",
    "\n",
    "- ***Distancia de Manhattan:*** Esta medida mide la suma de las diferencias absolutas en cada dimensión. En este caso, la distancia entre A y B es mayor que entre A y C, lo que sugiere que A y C son más similares en términos de sus características que A y B.\n",
    "\n",
    "- ***Distancia de Minkowski:*** Al utilizar $r=2$, esta distancia es equivalente a la distancia euclidiana. Los resultados aquí son similares a los de la distancia de Manhattan, donde la distancia A-B es mayor que la distancia A-C.\n",
    "\n",
    "- ***Similitud del Coseno:*** Los valores de similitud del coseno son más cercanos a 1 entre A y B (0.9953), lo que indica una mayor similitud en términos de orientación de las características. Entre A y C (0.9661), la similitud es ligeramente menor, lo que sugiere que A y B están más alineados que A y C en el espacio multidimensional."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39afe5b6",
   "metadata": {},
   "source": [
    "## Técnicas de identificación de registros raros (detección de outliers multivariantes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24ab661",
   "metadata": {},
   "source": [
    "### Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3929876f",
   "metadata": {},
   "source": [
    "En el análisis de datos empresariales, es común encontrarnos con situaciones en las que algunos registros se desvían significativamente del comportamiento típico de los demás. Estos registros inusuales, conocidos como outliers o valores atípicos, pueden distorsionar el análisis y la interpretación de los datos. Cuando trabajamos con múltiples variables interrelacionadas, es crucial emplear técnicas de detección de outliers multivariantes, ya que un enfoque univariante podría pasar por alto patrones complejos y relaciones entre las variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65de5dc9",
   "metadata": {},
   "source": [
    "### Definición de Outliers Multivariantes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b5f588",
   "metadata": {},
   "source": [
    "Un outlier multivariante es una observación en un conjunto de datos que se desvía significativamente de la tendencia general de las otras observaciones en términos de múltiples variables simultáneamente. Estas observaciones atípicas pueden ser el resultado de errores de medición, eventos raros o cambios en el proceso subyacente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0836caf4",
   "metadata": {},
   "source": [
    "### Métodos para detección de Outliers multivariantes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2750e677",
   "metadata": {},
   "source": [
    "Para la detección de registros raros se tiene una gran variedad de métodos. La elección del método dependerá de la naturaleza de los datos, los objetivos del análisis y la presencia de posibles observaciones atípicas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f6f2e5",
   "metadata": {},
   "source": [
    "#### Métodos basados en Distancia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4463cac4",
   "metadata": {},
   "source": [
    "Los métodos basados en la distancia son técnicas utilizadas para detectar outliers multivariantes al medir la similitud o la diferencia entre observaciones en un espacio de alta dimensión. Estos métodos se basan en la idea de que las observaciones atípicas tienden a estar más alejadas de la mayoría de las observaciones en términos de alguna medida de distancia. Estas técnicas son efectivas para identificar observaciones que se desvían significativamente de la estructura general de los datos, lo que podría indicar la presencia de outliers.\n",
    "\n",
    "- ***Distancia Euclidiana:*** La distancia euclidiana es la medida más común de distancia utilizada en análisis de datos. Se calcula como la raíz cuadrada de la suma de los cuadrados de las diferencias entre las coordenadas de dos puntos.\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "- ***[Distancia de Mahalanobis](https://en.wikipedia.org/wiki/Mahalanobis_distance):*** La distancia de Mahalanobis es una medida que ajusta la distancia euclidiana teniendo en cuenta la correlación entre las variables y la varianza de cada variable. Esto permite considerar la forma elipsoidal de la distribución de los datos.\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "- ***Distancia de Covarianza:*** La distancia de covarianza mide la diferencia entre las matrices de covarianza de dos observaciones. Es útil para detectar outliers que afectan la estructura de covarianza de los datos.\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "- ***Distancia de Correlación:*** La distancia de correlación mide la diferencia entre las matrices de correlación de dos observaciones. Puede ser útil para detectar outliers en datos altamente correlacionados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0781dcb",
   "metadata": {},
   "source": [
    "#### Métodos de Proyección"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b4c47e",
   "metadata": {},
   "source": [
    "Los métodos de proyección son técnicas que buscan transformar los datos originales desde un espacio de alta dimensión hacia un espacio de menor dimensión, mientras se conserva la mayor información posible. Estas técnicas son útiles para reducir la complejidad de los datos y visualizar patrones en dimensiones más manejables. Además, los métodos de proyección también pueden ser utilizados para detectar outliers multivariantes al resaltar las observaciones que se desvían significativamente de la estructura general de los datos.\n",
    "\n",
    "- ***[Análisis de Componentes Principales](https://en.wikipedia.org/wiki/Principal_component_analysis) (PCA):*** El PCA es una técnica de proyección que busca transformar los datos originales en un nuevo conjunto de variables no correlacionadas llamadas componentes principales. Cada componente principal captura una parte específica de la variabilidad en los datos. Los primeros componentes principales generalmente capturan la mayor varianza en los datos, y los puntos que se proyectan lejos del núcleo principal de puntos en el espacio de componentes principales podrían ser identificados como outliers.\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "- ***[Análisis de Componentes Independientes](https://en.wikipedia.org/wiki/Independent_component_analysis) (ICA):*** El ICA es similar al PCA, pero en lugar de buscar componentes no correlacionadas, busca componentes estadísticamente independientes. Esto lo hace útil para detectar patrones atípicos que podrían no ser detectados por el PCA.\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "- ***[Análisis de Factores](https://en.wikipedia.org/wiki/Factor_analysis) (FA):*** El FA es una técnica que busca identificar factores latentes que explican las relaciones subyacentes entre las variables observadas. Los outliers pueden afectar negativamente la identificación de estos factores y, por lo tanto, se pueden detectar si influyen en la capacidad del modelo para explicar la varianza observada.\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "- ***[Análisis de Coordenadas Principales](https://en.wikipedia.org/wiki/Multidimensional_scaling#Classical_multidimensional_scaling) (PCoA):*** El PCoA busca representar las relaciones de similitud o distancia entre observaciones en un espacio de menor dimensión. Las observaciones que se proyectan lejos del núcleo principal de observaciones en el espacio de coordenadas principales pueden considerarse outliers.\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "- ***Otras Técnicas de Reducción de Dimensionalidad:*** Además de los métodos mencionados, existen otras técnicas de reducción de dimensionalidad, como el Análisis Discriminante Lineal (LDA), que busca maximizar la separación entre diferentes clases en lugar de solo la varianza total, y el t-SNE (t-Distributed Stochastic Neighbor Embedding), que se utiliza principalmente para visualización."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42027e2",
   "metadata": {},
   "source": [
    "#### Métodos basados en Densidad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f483d36",
   "metadata": {},
   "source": [
    "Estos métodos buscan detectar outliers identificando áreas con baja densidad de datos en el espacio multidimensional. \n",
    "\n",
    "- ***[DBSCAN](https://en.wikipedia.org/wiki/DBSCAN) (Density-Based Spatial Clustering of Applications with Noise):*** es un algoritmo de agrupamiento que tiene la capacidad de detectar grupos de puntos en conjuntos de datos, incluidos aquellos que no tienen forma clara o no son linealmente separables. Aunque su enfoque principal es el clustering, el DBSCAN también puede ser utilizado para detectar outliers en los datos. La característica distintiva del algoritmo DBSCAN es que no requiere especificar el número de clusters de antemano. En su lugar, se basa en la densidad de puntos en el espacio para identificar regiones densas de puntos, considerando áreas con baja densidad como áreas que pueden contener outliers.\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "- ***DBSCAN Generalizado(GDBSCAN)***."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e19384",
   "metadata": {},
   "source": [
    "#### Métodos basados en Modelos Estadísticos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0683d942",
   "metadata": {},
   "source": [
    "Estos métodos utilizan modelos estadísticos para describir la estructura de los datos y luego identifican observaciones que se desvían significativamente de estos modelos. Entre otros se tienen: \n",
    "\n",
    "- ***[Regresión Robusta](https://en.wikipedia.org/wiki/Robust_regression):*** Se ajustan modelos de regresión que son menos sensibles a la influencia de outliers.\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "- ***Modelos de Mezcla:*** Se ajustan modelos que combinan varias distribuciones, y las observaciones que no se ajustan bien a ninguna distribución pueden considerarse outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bf7a12",
   "metadata": {},
   "source": [
    "#### Métodos basados en Aprendizaje Automático"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a46ddd",
   "metadata": {},
   "source": [
    "Estos métodos utilizan algoritmos de aprendizaje automático para identificar patrones inusuales en los datos.\n",
    "\n",
    "- ***[Isolation Forest](https://en.wikipedia.org/wiki/Isolation_forest):*** Este algoritmo crea un bosque de árboles de decisión y mide cuántos pasos se necesitan para aislar una observación. Las observaciones que se aíslan en menos pasos pueden considerarse outliers.\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "\n",
    "- ***[One-Class SVM (Support Vector Machine)](https://en.wikipedia.org/wiki/One-class_classification):*** Este método encuentra el hiperplano que mejor separa los datos y considera como outliers las observaciones que quedan en el lado opuesto del hiperplano."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9536be",
   "metadata": {},
   "source": [
    "#### Métodos basados en Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d6a12a",
   "metadata": {},
   "source": [
    "Estos métodos agrupan observaciones similares y pueden identificar observaciones que no se ajustan bien a ningún grupo.\n",
    "\n",
    "- ***[K-Means](https://en.wikipedia.org/wiki/K-means_clustering):*** Las observaciones que están lejos del centroide de su cluster pueden ser consideradas outliers.\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "- ***Cluster de Densidad:*** Similar al DBSCAN, agrupa puntos en áreas de alta densidad y puede identificar puntos aislados como outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbaceaa",
   "metadata": {},
   "source": [
    "#### Métodos basados en Robustez"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bc649d",
   "metadata": {},
   "source": [
    "Estos métodos están diseñados para ser resistentes a la influencia de observaciones atípicas en la detección de outliers.\n",
    "\n",
    "- ***[M-Estimadores](https://en.wikipedia.org/wiki/M-estimator):*** Son estimadores robustos que minimizan una función de error que es menos sensible a outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371f06fe",
   "metadata": {},
   "source": [
    "## Ejemplo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d21edf",
   "metadata": {},
   "source": [
    "<div class=\"alert alert alert-info\">\n",
    "Ver ejemplo en el archivo <a href=\"./Ejemplos_C03_5_Outliers_multivariate.ipynb\">Ejemplos_C03_5_Outliers_multivariate.ipynb</a>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
